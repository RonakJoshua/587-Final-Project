{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets trl peft bitsandbytes accelerate torch sentencepiece huggingface_hub -U\n",
    "!pip install --upgrade trl transformers accelerate\n",
    "!pip install --upgrade deepspeed==0.14.4\n",
    "\n",
    "# ATTENTION: Be sure to restart the notebook after installing all packages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.11/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "2025-04-22 13:06:12.850857: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-22 13:06:12.850920: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-22 13:06:12.852140: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-22 13:06:12.858894: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-22 13:06:13.636492: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-22 13:06:14,490] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.6\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.2.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @autocast_custom_fwd\n",
      "/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @autocast_custom_bwd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading base model with dtype: torch.bfloat16\n",
      "Loading base model: meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6578f6b3141e401a958fd2d1ff86c4cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for: meta-llama/Llama-3.2-3B-Instruct\n",
      "Loading LoRA adapter from: ./results_llama3_finetuned\n",
      "LoRA adapter loaded.\n",
      "Merging LoRA adapter into the base model...\n",
      "Adapter merged.\n",
      "\n",
      "Formatted Prompt:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Investigating the effects of manipulation on the gale shapley algorithm.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "Tokenizing prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response...\n",
      "\n",
      "================================================================================\n",
      "Model Response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Investigating the Effects of Manipulation on the Gale-Shapley Algorithm**\n",
       "\n",
       "The Gale-Shapley algorithm is a well-known algorithm used for stable marriage problems, which is a fundamental problem in combinatorial optimization and game theory. Manipulation refers to the phenomenon where an algorithm can be influenced by external factors (e.g., the order of proposals, the input order, or external constraints) to produce different stable matchings than the optimal stable matching. This can be undesirable in real-world applications where fairness and robustness are critical.\n",
       "\n",
       "### Research Problem:\n",
       "\n",
       "How does manipulation affect the performance and correctness of the Gale-Shapley algorithm? Specifically, under what conditions does the algorithm exhibit manipulation, and how can these effects be characterized or mitigated?\n",
       "\n",
       "### Hypothesis:\n",
       "\n",
       "Manipulation in the Gale-Shapley algorithm occurs when the algorithm's order of proposals or external factors influence the matchings in a way that deviates from the optimal stable matching. The extent and nature of manipulation depend on the input parameters such as the number of men and women, the preference lists, and the order of proposals.\n",
       "\n",
       "### Proposed Approach:\n",
       "\n",
       "1. **Literature Review:**\n",
       "   - Study existing theoretical results on the Gale-Shapley algorithm's stability and manipulation.\n",
       "   - Review prior empirical studies on manipulation in related algorithms.\n",
       "\n",
       "2. **Theoretical Analysis:**\n",
       "   - Analyze the algorithm's behavior under different proposal orders and external constraints.\n",
       "   - Characterize the conditions under which manipulation occurs.\n",
       "   - Use formal definitions and proofs to establish bounds on manipulation.\n",
       "\n",
       "3. **Simulation/Experiment:**\n",
       "   - Implement the Gale-Shapley algorithm with different proposal orders (e.g., random, reverse order, top-down, bottom-up).\n",
       "   - Generate synthetic datasets with known optimal stable matchings.\n",
       "   - Measure the deviation between algorithm outputs and optimal matchings due to manipulation.\n",
       "   - Analyze the impact of different input parameters (number of individuals, preference lists complexity).\n",
       "\n",
       "4. **Empirical Study:**\n",
       "   - Apply the algorithm to real-world datasets (e.g., university student matches, job assignments).\n",
       "   - Compare matches produced by the Gale-Shapley algorithm with and without manipulation.\n",
       "   - Collect qualitative feedback from domain experts or users on perceived fairness.\n",
       "\n",
       "### Data Collection Plan:\n",
       "\n",
       "- Synthetic data: Generate random preference lists and stable matchings for controlled experiments.\n",
       "- Real data: Obtain datasets from established repositories or partnerships (e.g., university matchmaker datasets).\n",
       "\n",
       "### Data Analysis Plan:\n",
       "\n",
       "- Quantitative analysis: Measure the degree of manipulation by comparing algorithm outputs with the true optimal matchings using metrics such as deviation from optimal cost or preference ordering.\n",
       "- Statistical analysis to test the significance of observed manipulation effects.\n",
       "- Qualitative analysis of expert feedback to understand perceived fairness.\n",
       "\n",
       "### Potential Challenges or Limitations:\n",
       "\n",
       "- Difficulty in formally defining and measuring \"manipulation\" in the algorithm.\n",
       "- Computational complexity of simulating all possible proposal orders and external constraints.\n",
       "- Generalizing results across different problem sizes and preference structures.\n",
       "- Ensuring the applicability of findings to diverse real-world settings.\n",
       "\n",
       "### Ethical Considerations:\n",
       "\n",
       "- If using real data, ensure privacy and confidentiality of participants.\n",
       "- Transparently communicate findings to avoid misuse of algorithmic fairness claims.\n",
       "- Consider implications of manipulation on vulnerable populations and fairness in matchmaking.\n",
       "\n",
       "---\n",
       "\n",
       "This approach aims to deepen understanding of manipulation in the Gale-Shapley algorithm, combining theoretical analysis, simulation, and empirical validation to improve the robustness and fairness of the algorithm."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import warnings\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Suppress warnings if needed\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Configuration ---\n",
    "base_model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# *** IMPORTANT: Update this path to where your LoRA adapter weights are saved ***\n",
    "adapter_path = \"./results_llama3_finetuned\"\n",
    "# Set device (use \"cuda\" if GPU is available, otherwise \"cpu\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Optional: Quantization config for loading base model (if needed for memory)\n",
    "# Adjust based on your inference hardware and how you trained/saved.\n",
    "# If you merge first, you might load the base without quantization here.\n",
    "use_quantization = False # Set to True if you want to load base model in 4-bit\n",
    "compute_dtype = torch.bfloat16 # Or torch.float16\n",
    "\n",
    "bnb_config = None\n",
    "if use_quantization:\n",
    "    print(\"Loading base model with 4-bit quantization...\")\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False, # Typically False for inference\n",
    "    )\n",
    "else:\n",
    "    print(f\"Loading base model with dtype: {compute_dtype}\")\n",
    "\n",
    "\n",
    "# --- Load Base Model and Tokenizer ---\n",
    "print(f\"Loading base model: {base_model_id}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=compute_dtype if not use_quantization else None, # Set dtype if not quantizing\n",
    "    device_map=\"auto\", # Automatically handle device placement\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading tokenizer for: {base_model_id}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Set pad token if needed\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# --- Load LoRA Adapter and Merge ---\n",
    "print(f\"Loading LoRA adapter from: {adapter_path}\")\n",
    "# Load the LoRA adapter weights on top of the base model\n",
    "# Note: device_map=\"auto\" should handle placing the adapter correctly\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "print(\"LoRA adapter loaded.\")\n",
    "\n",
    "print(\"Merging LoRA adapter into the base model...\")\n",
    "# Merge the adapter weights into the base model weights\n",
    "# This returns a standard transformers model object\n",
    "model = model.merge_and_unload()\n",
    "print(\"Adapter merged.\")\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# --- Prepare Prompt using the Correct Chat Template ---\n",
    "# *** CRITICAL: Use the exact same format as your training data ***\n",
    "# Example Llama 3.2 Instruct format (VERIFY THIS STRUCTURE!)\n",
    "prompt_text = \"Investigating the effects of manipulation on the gale shapley algorithm.\"\n",
    "\n",
    "# Format the prompt using the chat template structure\n",
    "# Ensure this matches the format_dataset_batch function from your training script\n",
    "formatted_prompt = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\" # Note the trailing space and newline for the assistant's turn\n",
    "\n",
    "print(\"\\nFormatted Prompt:\")\n",
    "print(formatted_prompt)\n",
    "\n",
    "# --- Tokenize Input ---\n",
    "print(\"Tokenizing prompt...\")\n",
    "# Encode the formatted prompt into token IDs\n",
    "# Ensure tensors are moved to the same device as the model\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", padding=False, truncation=True).to(device)\n",
    "\n",
    "# --- Generate Output ---\n",
    "print(\"Generating response...\")\n",
    "# Generate text using the model\n",
    "# Adjust generation parameters as needed (max_new_tokens, temperature, top_p, etc.)\n",
    "with torch.no_grad(): # Disable gradient calculations for inference\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=2048,          # Maximum number of new tokens to generate\n",
    "        do_sample=True,             # Whether to use sampling; set to False for deterministic output\n",
    "        temperature=0.6,            # Controls randomness (lower = more deterministic)\n",
    "        top_p=0.9,                  # Nucleus sampling probability\n",
    "        eos_token_id=tokenizer.eos_token_id # Stop generation when EOS token is encountered\n",
    "    )\n",
    "\n",
    "# --- Decode and Print Output ---\n",
    "# Decode the generated token IDs back into text\n",
    "# Skip special tokens (like padding or EOS) in the output string\n",
    "# We slice the output to only decode the newly generated tokens (after the input prompt)\n",
    "generated_token_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "response = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
    "\n",
    "div = \"=\"*80\n",
    "print(f\"\\n{div}\\nModel Response:\")\n",
    "display(Markdown(response))\n",
    "\n",
    "\n",
    "# print(response)\n",
    "\n",
    "print(\"\\nInference complete.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
