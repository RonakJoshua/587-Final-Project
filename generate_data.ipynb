{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import time\n",
    "from openai import AsyncOpenAI, OpenAIError # Use AsyncOpenAI for asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- Configuration ---\n",
    "load_dotenv()  # Load variables from .env file\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set.\")\n",
    "\n",
    "# Initialize the Async OpenAI client\n",
    "# It's good practice to initialize it once if possible,\n",
    "# but for simplicity in this function, we'll create it inside.\n",
    "# For applications with many calls, consider passing the client instance.\n",
    "# client = AsyncOpenAI(api_key=API_KEY)\n",
    "\n",
    "MODEL = \"gpt-4.1-mini\" # Or \"gpt-4\", \"gpt-4-turbo\", etc.\n",
    "MAX_TOKENS = 29000\n",
    "TEMPERATURE = 0.7\n",
    "MAX_CONCURRENT_REQUESTS = 500 # Adjust based on your rate limits and needs\n",
    "\n",
    "# Use a semaphore to limit concurrent requests\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "sys_prompt = \"\"\" **Input:** A clear description of the research problem or question.\n",
    "2. **Output:** A well-structured research approach. This could include:\n",
    "    - Hypothesis (if applicable)\n",
    "    - Proposed methodology (e.g., survey, experiment, simulation, case study, theoretical analysis)\n",
    "    - Key steps involved\n",
    "    - Data collection plan (if applicable)\n",
    "    - Data analysis plan (if applicable)\n",
    "    - Potential challenges or limitations\n",
    "    - Ethical considerations (if applicable)\"\"\"\n",
    "\n",
    "# --- Single Prompt Processing Function ---\n",
    "async def process_single_prompt(client: AsyncOpenAI, prompt: str, index: int) -> dict:\n",
    "    \"\"\"\n",
    "    Sends a single prompt to the OpenAI API asynchronously, handling errors.\n",
    "    Uses a semaphore to limit concurrency.\n",
    "    \"\"\"\n",
    "    async with semaphore: # Wait for semaphore if max concurrency is reached\n",
    "        print(f\"Processing prompt {index + 1}...\")\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = await client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=MAX_TOKENS,\n",
    "                temperature=TEMPERATURE,\n",
    "                # You can add other parameters like top_p, presence_penalty etc.\n",
    "            )\n",
    "            end_time = time.time()\n",
    "            result_content = response.choices[0].message.content.strip()\n",
    "            print(f\"Finished prompt {index + 1} in {end_time - start_time:.2f}s ({response.usage.completion_tokens} tkns)\")\n",
    "            return {\n",
    "                \"index\": index,\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": result_content,\n",
    "                \"status\": \"success\",\n",
    "                \"error\": None,\n",
    "                \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "                \"completion_tokens\": response.usage.completion_tokens\n",
    "            }\n",
    "        except OpenAIError as e:\n",
    "            print(f\"Error processing prompt {index + 1}: {e}\")\n",
    "            return {\n",
    "                \"index\": index,\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": None,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e),\n",
    "                \"prompt_tokens\": -1,\n",
    "                \"completion_tokens\": -1\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error processing prompt {index + 1}: {e}\")\n",
    "            return {\n",
    "                \"index\": index,\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": None,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": f\"Unexpected error: {str(e)}\",\n",
    "                \"prompt_tokens\": -1,\n",
    "                \"completion_tokens\": -1\n",
    "            }\n",
    "\n",
    "# --- Batch Pipeline Function ---\n",
    "async def batch_openai_pipeline(prompts: list[str]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Processes a list of prompts concurrently using the OpenAI API.\n",
    "    \"\"\"\n",
    "    # Initialize client here for the batch duration\n",
    "    async_client = AsyncOpenAI(api_key=API_KEY)\n",
    "\n",
    "    # Create a list of tasks for asyncio to run\n",
    "    # Pass the client instance to each task\n",
    "    tasks = [\n",
    "        process_single_prompt(async_client, prompt, i)\n",
    "        for i, prompt in enumerate(prompts)\n",
    "    ]\n",
    "\n",
    "    # Run tasks concurrently and gather results\n",
    "    # return_exceptions=True ensures that if one task fails, others continue,\n",
    "    # and the exception is returned in the results list instead of crashing.\n",
    "    print(f\"Starting batch processing for {len(prompts)} prompts...\")\n",
    "    start_batch_time = time.time()\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    end_batch_time = time.time()\n",
    "    print(f\"Batch processing finished in {end_batch_time - start_batch_time:.2f}s\")\n",
    "\n",
    "    # Process results to handle potential exceptions returned by gather\n",
    "    final_results = []\n",
    "    for i, result in enumerate(results):\n",
    "        if isinstance(result, Exception):\n",
    "            print(f\"Task for prompt {i+1} failed with an exception: {result}\")\n",
    "            # You might want to create a specific error entry here\n",
    "            final_results.append({\n",
    "                \"index\": i,\n",
    "                \"prompt\": prompts[i], # Get original prompt\n",
    "                \"response\": None,\n",
    "                \"status\": \"gather_error\",\n",
    "                \"error\": f\"Gather exception: {str(result)}\"\n",
    "            })\n",
    "        else:\n",
    "            final_results.append(result) # Append the dictionary returned by process_single_prompt\n",
    "\n",
    "    # Sort results by original index to maintain order\n",
    "    final_results.sort(key=lambda x: x['index'])\n",
    "\n",
    "    return final_results\n",
    "\n",
    "# --- Example Usage ---\n",
    "# async def main():\n",
    "#     # sample_prompts = [\n",
    "#     #     \"Explain the concept of asynchronous programming in Python.\",\n",
    "#     #     \"Write a short poem about a rainy day.\",\n",
    "#     #     \"What is the capital of France?\",\n",
    "#     #     \"Summarize the main plot points of 'Hamlet'.\",\n",
    "#     #     \"Translate 'Hello, world!' to Spanish.\",\n",
    "#     #     \"What are the benefits of using a semaphore?\",\n",
    "#     #     \"Give me three healthy breakfast ideas.\",\n",
    "#     #     \"Explain the difference between GPT-3.5 and GPT-4.\",\n",
    "#     #     \"Write a python function to calculate factorial.\",\n",
    "#     #     \"Who painted the Mona Lisa?\"\n",
    "#     #     # Add more prompts as needed\n",
    "#     # ]\n",
    "#     sample_prompts = [\n",
    "#         \"Generate 5 different novel research problems in the field of psychology. Denote each question with <problem></problem> tags.\",\n",
    "#     ]\n",
    "\n",
    "#     results = await batch_openai_pipeline(sample_prompts)\n",
    "\n",
    "#     print(\"\\n--- Batch Results ---\")\n",
    "#     for result in results:\n",
    "#         print(f\"\\nPrompt {result['index'] + 1}: {result['prompt']}\")\n",
    "#         if result['status'] == 'success':\n",
    "#             print(f\"Response: {result['response']}\")\n",
    "#             print(result['completion_tokens'])\n",
    "#         else:\n",
    "#             print(f\"Status: {result['status']}\")\n",
    "#             print(f\"Error: {result['error']}\")\n",
    "#         print(\"-\" * 20)\n",
    "\n",
    "#     return results\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # To run the async main function\n",
    "#     # asyncio.run(main())\n",
    "#     x = await main()\n",
    "\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts(topics):\n",
    "    prompts = []\n",
    "    for topic in topics:\n",
    "        prompts.append(f\"Generate 200 different novel academic research problems in the field of {topic}. Denote each question with <problem></problem> tags.\")\n",
    "    return prompts\n",
    "\n",
    "async def main(prompts):\n",
    "    # sample_prompts = [\n",
    "    #     \"Generate 5 different novel research problems in the field of psychology. Denote each question with <problem></problem> tags.\",\n",
    "    # ]\n",
    "\n",
    "    results = await batch_openai_pipeline(prompts)\n",
    "\n",
    "    print(\"\\n--- Batch Results ---\")\n",
    "    for result in results:\n",
    "        print(f\"\\nPrompt {result['index'] + 1}: {result['prompt']}\")\n",
    "        if result['status'] == 'success':\n",
    "            print(f\"Response: {result['response']}\")\n",
    "            print(result['completion_tokens'])\n",
    "        else:\n",
    "            print(f\"Status: {result['status']}\")\n",
    "            print(f\"Error: {result['error']}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "    return results\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # To run the async main function\n",
    "#     # asyncio.run(main())\n",
    "#     x = await main()\n",
    "\n",
    "disciplines = [\n",
    "    # --- Natural Sciences ---\n",
    "    \"Physics\",\n",
    "    \"Chemistry\",\n",
    "    \"Biology\",\n",
    "    \"Earth Sciences\",  # (Geology, Oceanography, Meteorology)\n",
    "    \"Astronomy\",\n",
    "    \"Environmental Science\",\n",
    "    \"Materials Science\",\n",
    "\n",
    "    # --- Life Sciences / Medicine ---\n",
    "    \"Medicine\",\n",
    "    \"Neuroscience\",\n",
    "    \"Genetics\",\n",
    "    \"Ecology\",\n",
    "    \"Molecular Biology\",\n",
    "    \"Biochemistry\",\n",
    "    \"Pharmacology\",\n",
    "    \"Public Health\",\n",
    "    \"Nursing\",\n",
    "    \"Biomedical Engineering\", # Straddles Engineering/Medicine\n",
    "\n",
    "    # --- Formal Sciences ---\n",
    "    \"Mathematics\",\n",
    "    \"Statistics\",\n",
    "    \"Computer Science\",\n",
    "    \"Logic\", # Often within Philosophy or Math\n",
    "    \"Data Science\", # Interdisciplinary but distinct focus\n",
    "\n",
    "    # --- Social Sciences ---\n",
    "    \"Psychology\",\n",
    "    \"Sociology\",\n",
    "    \"Anthropology\",\n",
    "    \"Economics\",\n",
    "    \"Political Science\",\n",
    "    \"Geography\", # (Human Geography focus)\n",
    "    \"Communication Studies\",\n",
    "    \"Education\",\n",
    "    \"Archaeology\", # Straddles Humanities/Social Sciences\n",
    "\n",
    "    # --- Humanities ---\n",
    "    \"History\",\n",
    "    \"Philosophy\",\n",
    "    \"Literature\", # (English, Comparative, etc.)\n",
    "    \"Linguistics\",\n",
    "    \"Religious Studies\",\n",
    "    \"Classics\", # (Study of ancient Greece and Rome)\n",
    "    \"Art History\",\n",
    "\n",
    "    # --- Engineering & Technology ---\n",
    "    \"Mechanical Engineering\",\n",
    "    \"Electrical Engineering\",\n",
    "    \"Civil Engineering\",\n",
    "    \"Chemical Engineering\",\n",
    "    \"Aerospace Engineering\",\n",
    "    \"Industrial Engineering\",\n",
    "    \"Software Engineering\", # Often within CS but distinct focus\n",
    "    \"Robotics\", # Interdisciplinary\n",
    "\n",
    "    # --- Arts ---\n",
    "    \"Visual Arts\", # (Painting, Sculpture, Photography etc.)\n",
    "    \"Music\", # (Musicology, Composition, Performance Studies)\n",
    "    \"Theatre Studies\",\n",
    "    \"Film Studies\",\n",
    "    \"Architecture\",\n",
    "    \"Design\", # (Graphic Design, Industrial Design, Interaction Design)\n",
    "\n",
    "    # --- Professional Fields ---\n",
    "    \"Law\",\n",
    "    \"Business Administration\", # (Management, Strategy)\n",
    "    \"Finance\",\n",
    "    \"Marketing\",\n",
    "    \"Accounting\",\n",
    "    \"Urban Planning\",\n",
    "    \"Library and Information Science\",\n",
    "    \"Social Work\",\n",
    "    \"Agricultural Science\",\n",
    "]\n",
    "\n",
    "responses = await main(generate_prompts(disciplines))\n",
    "\n",
    "print(responses)\n",
    "\n",
    "# Optional: Print the number of disciplines listed\n",
    "# print(f\"Generated a list of {len(disciplines)} disciplines.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_question_list = []\n",
    "topic_list = []\n",
    "for idx, r in enumerate(responses):\n",
    "    output = r['response']\n",
    "    while \"<problem>\" in output:\n",
    "        problem = output[output.rfind('<problem>')+9:output.rfind('</problem>')]\n",
    "        # print(problem)\n",
    "        output = output[:output.rfind('<problem>')]\n",
    "        research_question_list.append(problem)\n",
    "        topic_list.append(disciplines[idx])\n",
    "\n",
    "question_data = {\"prompt\": topic_list, \"completion\": research_question_list}\n",
    "print(question_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(question_data)\n",
    "df.to_csv('topic_question.csv', index=False)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = await main(df['prompt'].to_list())\n",
    "\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approach_list = []\n",
    "question_list = []\n",
    "for idx, r in enumerate(responses):\n",
    "    approach_list.append(r['response'])\n",
    "    question_list.append(r['prompt'])\n",
    "    # while \"<problem>\" in output:\n",
    "    #     problem = output[output.rfind('<problem>')+9:output.rfind('</problem>')]\n",
    "    #     # print(problem)\n",
    "    #     output = output[:output.rfind('<problem>')]\n",
    "    #     research_question_list.append(problem)\n",
    "    #     topic_list.append(disciplines[idx])\n",
    "\n",
    "approach_data = {\"prompt\": question_list, \"completion\": approach_list}\n",
    "\n",
    "df_approach = pd.DataFrame.from_dict(approach_data)\n",
    "\n",
    "df_combined = pd.merge(df, df_approach, on='question')\n",
    "\n",
    "# df_approach\n",
    "\n",
    "df_combined.to_csv('topic_question_approach.csv', index=False)\n",
    "\n",
    "# Find duplicates in 'col1', keeping the first occurrence\n",
    "duplicates_dropped = df_combined.drop_duplicates(subset=['prompt'])\n",
    "\n",
    "# print(\"Duplicates (keep='first'):\\n\", duplicates)\n",
    "\n",
    "# Filter the DataFrame to show duplicate rows\n",
    "\n",
    "duplicates_dropped.to_csv('topic_question_approach.csv', index=False)\n",
    "duplicates_dropped\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import random\n",
    "\n",
    "data_points = random.randint(50, 200)\n",
    "threshold = 100\n",
    "status = \"**above**\" if data_points > threshold else \"*below*\"\n",
    "\n",
    "# Display the string as rendered Markdown\n",
    "# display(Markdown(duplicates_dropped.iloc[0]['approach']))\n",
    "\n",
    "loaded_df = pd.read_csv('topic_question_approach.csv')\n",
    "\n",
    "none_values = loaded_df[loaded_df.isnull().any(axis=1)]\n",
    "\n",
    "responses = await main(none_values['prompt'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(responses)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Create a copy to avoid modifying the original df if running multiple methods\n",
    "df_loop_method = loaded_df.copy()\n",
    "\n",
    "# Iterate through the list of dictionaries\n",
    "for qa_item in responses:\n",
    "    question_to_find = qa_item['prompt']\n",
    "    answer_to_add = qa_item['response']\n",
    "\n",
    "    # Find rows where the question matches and update the 'answer' column\n",
    "    # .loc[row_indexer, column_indexer] = value\n",
    "    df_loop_method.loc[df_loop_method['prompt'] == question_to_find, 'completion'] = answer_to_add\n",
    "\n",
    "print(\"\\nDataFrame Updated using loop and .loc:\")\n",
    "df_loop_method\n",
    "\n",
    "df_loop_method.to_csv('topic_question_approach.csv', index=False)\n",
    "\n",
    "truncated_dataset = df_loop_method.groupby('topic', group_keys=False).head(150)\n",
    "\n",
    "truncated_dataset.to_csv('topic_question_approach_trunc.csv', index=False)\n",
    "\n",
    "truncated_dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
